{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax exercise\n",
    "\n",
    "*Complete and hand in this completed worksheet (including its outputs and any supporting code outside of the worksheet) with your assignment submission. For more details see the [assignments page](http://vision.stanford.edu/teaching/cs231n/assignments.html) on the course website.*\n",
    "\n",
    "This exercise is analogous to the SVM exercise. You will:\n",
    "\n",
    "- implement a fully-vectorized **loss function** for the Softmax classifier\n",
    "- implement the fully-vectorized expression for its **analytic gradient**\n",
    "- **check your implementation** with numerical gradient\n",
    "- use a validation set to **tune the learning rate and regularization** strength\n",
    "- **optimize** the loss function with **SGD**\n",
    "- **visualize** the final learned weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from cs231n.data_utils import load_CIFAR10\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading extenrnal modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clear previously loaded data.\n",
      "Train data shape:  (49000, 3073)\n",
      "Train labels shape:  (49000,)\n",
      "Validation data shape:  (1000, 3073)\n",
      "Validation labels shape:  (1000,)\n",
      "Test data shape:  (1000, 3073)\n",
      "Test labels shape:  (1000,)\n",
      "dev data shape:  (500, 3073)\n",
      "dev labels shape:  (500,)\n"
     ]
    }
   ],
   "source": [
    "def get_CIFAR10_data(num_training=49000, num_validation=1000, num_test=1000, num_dev=500):\n",
    "    \"\"\"\n",
    "    Load the CIFAR-10 dataset from disk and perform preprocessing to prepare\n",
    "    it for the linear classifier. These are the same steps as we used for the\n",
    "    SVM, but condensed to a single function.  \n",
    "    \"\"\"\n",
    "    # Load the raw CIFAR-10 data\n",
    "    cifar10_dir = 'cs231n/datasets/cifar-10-batches-py'\n",
    "    \n",
    "    X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)\n",
    "    \n",
    "    # subsample the data\n",
    "    mask = list(range(num_training, num_training + num_validation))\n",
    "    X_val = X_train[mask]\n",
    "    y_val = y_train[mask]\n",
    "    mask = list(range(num_training))\n",
    "    X_train = X_train[mask]\n",
    "    y_train = y_train[mask]\n",
    "    mask = list(range(num_test))\n",
    "    X_test = X_test[mask]\n",
    "    y_test = y_test[mask]\n",
    "    mask = np.random.choice(num_training, num_dev, replace=False)\n",
    "    X_dev = X_train[mask]\n",
    "    y_dev = y_train[mask]\n",
    "    \n",
    "    # Preprocessing: reshape the image data into rows\n",
    "    X_train = np.reshape(X_train, (X_train.shape[0], -1))\n",
    "    X_val = np.reshape(X_val, (X_val.shape[0], -1))\n",
    "    X_test = np.reshape(X_test, (X_test.shape[0], -1))\n",
    "    X_dev = np.reshape(X_dev, (X_dev.shape[0], -1))\n",
    "    \n",
    "    # Normalize the data: subtract the mean image\n",
    "    mean_image = np.mean(X_train, axis = 0)\n",
    "    X_train -= mean_image\n",
    "    X_val -= mean_image\n",
    "    X_test -= mean_image\n",
    "    X_dev -= mean_image\n",
    "    \n",
    "    # add bias dimension and transform into columns\n",
    "    X_train = np.hstack([X_train, np.ones((X_train.shape[0], 1))])\n",
    "    X_val = np.hstack([X_val, np.ones((X_val.shape[0], 1))])\n",
    "    X_test = np.hstack([X_test, np.ones((X_test.shape[0], 1))])\n",
    "    X_dev = np.hstack([X_dev, np.ones((X_dev.shape[0], 1))])\n",
    "    \n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test, X_dev, y_dev\n",
    "\n",
    "\n",
    "# Cleaning up variables to prevent loading data multiple times (which may cause memory issue)\n",
    "try:\n",
    "   del X_train, y_train\n",
    "   del X_test, y_test\n",
    "   print('Clear previously loaded data.')\n",
    "except:\n",
    "   pass\n",
    "\n",
    "# Invoke the above function to get our data.\n",
    "X_train, y_train, X_val, y_val, X_test, y_test, X_dev, y_dev = get_CIFAR10_data()\n",
    "print('Train data shape: ', X_train.shape)\n",
    "print('Train labels shape: ', y_train.shape)\n",
    "print('Validation data shape: ', X_val.shape)\n",
    "print('Validation labels shape: ', y_val.shape)\n",
    "print('Test data shape: ', X_test.shape)\n",
    "print('Test labels shape: ', y_test.shape)\n",
    "print('dev data shape: ', X_dev.shape)\n",
    "print('dev labels shape: ', y_dev.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax Classifier\n",
    "\n",
    "Your code for this section will all be written inside **cs231n/classifiers/softmax.py**. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 2.364007\n",
      "sanity check: 2.302585\n"
     ]
    }
   ],
   "source": [
    "# First implement the naive softmax loss function with nested loops.\n",
    "# Open the file cs231n/classifiers/softmax.py and implement the\n",
    "# softmax_loss_naive function.\n",
    "\n",
    "from cs231n.classifiers.softmax import softmax_loss_naive\n",
    "import time\n",
    "\n",
    "# Generate a random softmax weight matrix and use it to compute the loss.\n",
    "W = np.random.randn(3073, 10) * 0.0001\n",
    "loss, grad = softmax_loss_naive(W, X_dev, y_dev, 0.0)\n",
    "\n",
    "# As a rough sanity check, our loss should be something close to -log(0.1).\n",
    "print('loss: %f' % loss)\n",
    "print('sanity check: %f' % (-np.log(0.1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inline Question 1:\n",
    "Why do we expect our loss to be close to -log(0.1)? Explain briefly.**\n",
    "\n",
    "**Your answer:** There are 10 classes, each with a weight calculated using the same random number generation, so each weight will on average be 1/10 of the total value, and then also 1/10 once they are exponentiated, so the loss, calculated by taking that value and dividing by the total, and then taking the negative log, will be about -log(1/10), when they are averaged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numerical: -1.708597 analytic: -1.708597, relative error: 1.694494e-08\n",
      "numerical: -0.176186 analytic: -0.176186, relative error: 2.439949e-07\n",
      "numerical: 1.373252 analytic: 1.373252, relative error: 5.248213e-08\n",
      "numerical: 1.889585 analytic: 1.889585, relative error: 8.429665e-10\n",
      "numerical: 1.069360 analytic: 1.069360, relative error: 5.369128e-08\n",
      "numerical: -0.213683 analytic: -0.213683, relative error: 9.924525e-08\n",
      "numerical: 1.777028 analytic: 1.777028, relative error: 2.379077e-09\n",
      "numerical: 2.304623 analytic: 2.304623, relative error: 7.616444e-09\n",
      "numerical: -0.158570 analytic: -0.158570, relative error: 1.258064e-07\n",
      "numerical: -0.667958 analytic: -0.667958, relative error: 6.581799e-08\n",
      "numerical: 3.162542 analytic: 3.162542, relative error: 1.205901e-08\n",
      "numerical: -4.204418 analytic: -4.204418, relative error: 1.427808e-08\n",
      "numerical: 0.249176 analytic: 0.249176, relative error: 1.226591e-07\n",
      "numerical: -3.813482 analytic: -3.813482, relative error: 5.875403e-11\n",
      "numerical: 1.872882 analytic: 1.872882, relative error: 5.884940e-10\n",
      "numerical: 0.783162 analytic: 0.783162, relative error: 9.891712e-09\n",
      "numerical: 0.983802 analytic: 0.983802, relative error: 1.077667e-08\n",
      "numerical: 0.775897 analytic: 0.775897, relative error: 6.002244e-08\n",
      "numerical: -5.810809 analytic: -5.810809, relative error: 4.895346e-09\n",
      "numerical: -2.272898 analytic: -2.272898, relative error: 1.216447e-08\n"
     ]
    }
   ],
   "source": [
    "# Complete the implementation of softmax_loss_naive and implement a (naive)\n",
    "# version of the gradient that uses nested loops.\n",
    "loss, grad = softmax_loss_naive(W, X_dev, y_dev, 0.0)\n",
    "\n",
    "# As we did for the SVM, use numeric gradient checking as a debugging tool.\n",
    "# The numeric gradient should be close to the analytic gradient.\n",
    "from cs231n.gradient_check import grad_check_sparse\n",
    "f = lambda w: softmax_loss_naive(w, X_dev, y_dev, 0.0)[0]\n",
    "grad_numerical = grad_check_sparse(f, W, grad, 10)\n",
    "\n",
    "# similar to SVM case, do another gradient check with regularization\n",
    "loss, grad = softmax_loss_naive(W, X_dev, y_dev, 5e1)\n",
    "f = lambda w: softmax_loss_naive(w, X_dev, y_dev, 5e1)[0]\n",
    "grad_numerical = grad_check_sparse(f, W, grad, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "naive loss: 2.364007e+00 computed in 0.101080s\n",
      "vectorized loss: 2.364007e+00 computed in 0.001915s\n",
      "Loss difference: 0.000000\n",
      "Gradient difference: 0.000000\n"
     ]
    }
   ],
   "source": [
    "# Now that we have a naive implementation of the softmax loss function and its gradient,\n",
    "# implement a vectorized version in softmax_loss_vectorized.\n",
    "# The two versions should compute the same results, but the vectorized version should be\n",
    "# much faster.\n",
    "tic = time.time()\n",
    "loss_naive, grad_naive = softmax_loss_naive(W, X_dev, y_dev, 0.000005)\n",
    "toc = time.time()\n",
    "print('naive loss: %e computed in %fs' % (loss_naive, toc - tic))\n",
    "\n",
    "from cs231n.classifiers.softmax import softmax_loss_vectorized\n",
    "tic = time.time()\n",
    "loss_vectorized, grad_vectorized = softmax_loss_vectorized(W, X_dev, y_dev, 0.000005)\n",
    "toc = time.time()\n",
    "print('vectorized loss: %e computed in %fs' % (loss_vectorized, toc - tic))\n",
    "\n",
    "# As we did for the SVM, we use the Frobenius norm to compare the two versions\n",
    "# of the gradient.\n",
    "grad_difference = np.linalg.norm(grad_naive - grad_vectorized, ord='fro')\n",
    "print('Loss difference: %f' % np.abs(loss_naive - loss_vectorized))\n",
    "print('Gradient difference: %f' % grad_difference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 / 1500: loss 308.253286\n",
      "iteration 100 / 1500: loss 7.289980\n",
      "iteration 200 / 1500: loss 2.056486\n",
      "iteration 300 / 1500: loss 2.094063\n",
      "iteration 400 / 1500: loss 1.992971\n",
      "iteration 500 / 1500: loss 1.981794\n",
      "iteration 600 / 1500: loss 1.990604\n",
      "iteration 700 / 1500: loss 2.035297\n",
      "iteration 800 / 1500: loss 1.986058\n",
      "iteration 900 / 1500: loss 1.929933\n",
      "iteration 1000 / 1500: loss 1.985861\n",
      "iteration 1100 / 1500: loss 1.998403\n",
      "iteration 1200 / 1500: loss 2.080851\n",
      "iteration 1300 / 1500: loss 1.936568\n",
      "iteration 1400 / 1500: loss 1.963213\n",
      "iteration 0 / 1500: loss 621.488242\n",
      "iteration 100 / 1500: loss 2.193836\n",
      "iteration 200 / 1500: loss 2.130475\n",
      "iteration 300 / 1500: loss 2.037328\n",
      "iteration 400 / 1500: loss 2.048484\n",
      "iteration 500 / 1500: loss 2.049028\n",
      "iteration 600 / 1500: loss 2.084700\n",
      "iteration 700 / 1500: loss 2.106748\n",
      "iteration 800 / 1500: loss 2.036836\n",
      "iteration 900 / 1500: loss 2.032249\n",
      "iteration 1000 / 1500: loss 2.116106\n",
      "iteration 1100 / 1500: loss 2.079506\n",
      "iteration 1200 / 1500: loss 2.089776\n",
      "iteration 1300 / 1500: loss 2.116425\n",
      "iteration 1400 / 1500: loss 2.095783\n",
      "iteration 0 / 1500: loss 921.376774\n",
      "iteration 100 / 1500: loss 2.111469\n",
      "iteration 200 / 1500: loss 2.138964\n",
      "iteration 300 / 1500: loss 2.054753\n",
      "iteration 400 / 1500: loss 2.120688\n",
      "iteration 500 / 1500: loss 2.147978\n",
      "iteration 600 / 1500: loss 2.155304\n",
      "iteration 700 / 1500: loss 2.068678\n",
      "iteration 800 / 1500: loss 2.118767\n",
      "iteration 900 / 1500: loss 2.070614\n",
      "iteration 1000 / 1500: loss 2.163641\n",
      "iteration 1100 / 1500: loss 2.087474\n",
      "iteration 1200 / 1500: loss 2.098809\n",
      "iteration 1300 / 1500: loss 2.148956\n",
      "iteration 1400 / 1500: loss 2.062567\n",
      "iteration 0 / 1500: loss 1233.942345\n",
      "iteration 100 / 1500: loss 2.156051\n",
      "iteration 200 / 1500: loss 2.119572\n",
      "iteration 300 / 1500: loss 2.122500\n",
      "iteration 400 / 1500: loss 2.110836\n",
      "iteration 500 / 1500: loss 2.120263\n",
      "iteration 600 / 1500: loss 2.160069\n",
      "iteration 700 / 1500: loss 2.126361\n",
      "iteration 800 / 1500: loss 2.123806\n",
      "iteration 900 / 1500: loss 2.129371\n",
      "iteration 1000 / 1500: loss 2.169525\n",
      "iteration 1100 / 1500: loss 2.192972\n",
      "iteration 1200 / 1500: loss 2.103064\n",
      "iteration 1300 / 1500: loss 2.121955\n",
      "iteration 1400 / 1500: loss 2.126672\n",
      "iteration 0 / 1500: loss 1567.028756\n",
      "iteration 100 / 1500: loss 2.143028\n",
      "iteration 200 / 1500: loss 2.156048\n",
      "iteration 300 / 1500: loss 2.151905\n",
      "iteration 400 / 1500: loss 2.169440\n",
      "iteration 500 / 1500: loss 2.152168\n",
      "iteration 600 / 1500: loss 2.167915\n",
      "iteration 700 / 1500: loss 2.125516\n",
      "iteration 800 / 1500: loss 2.156025\n",
      "iteration 900 / 1500: loss 2.188072\n",
      "iteration 1000 / 1500: loss 2.159631\n",
      "iteration 1100 / 1500: loss 2.147552\n",
      "iteration 1200 / 1500: loss 2.163613\n",
      "iteration 1300 / 1500: loss 2.144683\n",
      "iteration 1400 / 1500: loss 2.143483\n",
      "iteration 0 / 1500: loss 309.553985\n",
      "iteration 100 / 1500: loss 9.937763\n",
      "iteration 200 / 1500: loss 2.241111\n",
      "iteration 300 / 1500: loss 1.975563\n",
      "iteration 400 / 1500: loss 1.992919\n",
      "iteration 500 / 1500: loss 1.946816\n",
      "iteration 600 / 1500: loss 2.053243\n",
      "iteration 700 / 1500: loss 2.005583\n",
      "iteration 800 / 1500: loss 2.013052\n",
      "iteration 900 / 1500: loss 1.944922\n",
      "iteration 1000 / 1500: loss 1.916570\n",
      "iteration 1100 / 1500: loss 1.935866\n",
      "iteration 1200 / 1500: loss 2.014857\n",
      "iteration 1300 / 1500: loss 1.980268\n",
      "iteration 1400 / 1500: loss 2.030534\n",
      "iteration 0 / 1500: loss 628.019686\n",
      "iteration 100 / 1500: loss 2.462203\n",
      "iteration 200 / 1500: loss 2.052053\n",
      "iteration 300 / 1500: loss 2.099978\n",
      "iteration 400 / 1500: loss 2.095834\n",
      "iteration 500 / 1500: loss 2.117246\n",
      "iteration 600 / 1500: loss 2.070845\n",
      "iteration 700 / 1500: loss 1.991299\n",
      "iteration 800 / 1500: loss 2.053792\n",
      "iteration 900 / 1500: loss 2.085501\n",
      "iteration 1000 / 1500: loss 2.068427\n",
      "iteration 1100 / 1500: loss 2.080595\n",
      "iteration 1200 / 1500: loss 2.056692\n",
      "iteration 1300 / 1500: loss 2.085304\n",
      "iteration 1400 / 1500: loss 2.110647\n",
      "iteration 0 / 1500: loss 928.500359\n",
      "iteration 100 / 1500: loss 2.135825\n",
      "iteration 200 / 1500: loss 2.119641\n",
      "iteration 300 / 1500: loss 2.148978\n",
      "iteration 400 / 1500: loss 2.088232\n",
      "iteration 500 / 1500: loss 2.147321\n",
      "iteration 600 / 1500: loss 2.110613\n",
      "iteration 700 / 1500: loss 2.046628\n",
      "iteration 800 / 1500: loss 2.113106\n",
      "iteration 900 / 1500: loss 2.146703\n",
      "iteration 1000 / 1500: loss 2.036129\n",
      "iteration 1100 / 1500: loss 2.084895\n",
      "iteration 1200 / 1500: loss 2.191575\n",
      "iteration 1300 / 1500: loss 2.122856\n",
      "iteration 1400 / 1500: loss 2.144897\n",
      "iteration 0 / 1500: loss 1241.108351\n",
      "iteration 100 / 1500: loss 2.154141\n",
      "iteration 200 / 1500: loss 2.122321\n",
      "iteration 300 / 1500: loss 2.152690\n",
      "iteration 400 / 1500: loss 2.139698\n",
      "iteration 500 / 1500: loss 2.159548\n",
      "iteration 600 / 1500: loss 2.098631\n",
      "iteration 700 / 1500: loss 2.140362\n",
      "iteration 800 / 1500: loss 2.142847\n",
      "iteration 900 / 1500: loss 2.163403\n",
      "iteration 1000 / 1500: loss 2.136137\n",
      "iteration 1100 / 1500: loss 2.146806\n",
      "iteration 1200 / 1500: loss 2.095585\n",
      "iteration 1300 / 1500: loss 2.090914\n",
      "iteration 1400 / 1500: loss 2.044541\n",
      "iteration 0 / 1500: loss 1550.873518\n",
      "iteration 100 / 1500: loss 2.148386\n",
      "iteration 200 / 1500: loss 2.144872\n",
      "iteration 300 / 1500: loss 2.165665\n",
      "iteration 400 / 1500: loss 2.085696\n",
      "iteration 500 / 1500: loss 2.110628\n",
      "iteration 600 / 1500: loss 2.185752\n",
      "iteration 700 / 1500: loss 2.145143\n",
      "iteration 800 / 1500: loss 2.138661\n",
      "iteration 900 / 1500: loss 2.182943\n",
      "iteration 1000 / 1500: loss 2.131809\n",
      "iteration 1100 / 1500: loss 2.105481\n",
      "iteration 1200 / 1500: loss 2.209969\n",
      "iteration 1300 / 1500: loss 2.142145\n",
      "iteration 1400 / 1500: loss 2.173043\n",
      "iteration 0 / 1500: loss 312.022343\n",
      "iteration 100 / 1500: loss 14.037414\n",
      "iteration 200 / 1500: loss 2.384641\n",
      "iteration 300 / 1500: loss 2.120852\n",
      "iteration 400 / 1500: loss 1.950320\n",
      "iteration 500 / 1500: loss 2.055165\n",
      "iteration 600 / 1500: loss 1.991071\n",
      "iteration 700 / 1500: loss 2.008053\n",
      "iteration 800 / 1500: loss 1.983360\n",
      "iteration 900 / 1500: loss 2.017554\n",
      "iteration 1000 / 1500: loss 2.050105\n",
      "iteration 1100 / 1500: loss 2.002031\n",
      "iteration 1200 / 1500: loss 2.030304\n",
      "iteration 1300 / 1500: loss 2.062580\n",
      "iteration 1400 / 1500: loss 1.959706\n",
      "iteration 0 / 1500: loss 616.313616\n",
      "iteration 100 / 1500: loss 2.984525\n",
      "iteration 200 / 1500: loss 1.987499\n",
      "iteration 300 / 1500: loss 2.089706\n",
      "iteration 400 / 1500: loss 2.046605\n",
      "iteration 500 / 1500: loss 2.058246\n",
      "iteration 600 / 1500: loss 2.104936\n",
      "iteration 700 / 1500: loss 2.049858\n",
      "iteration 800 / 1500: loss 2.007833\n",
      "iteration 900 / 1500: loss 2.074742\n",
      "iteration 1000 / 1500: loss 2.019067\n",
      "iteration 1100 / 1500: loss 2.157153\n",
      "iteration 1200 / 1500: loss 2.035583\n",
      "iteration 1300 / 1500: loss 2.067050\n",
      "iteration 1400 / 1500: loss 2.095161\n",
      "iteration 0 / 1500: loss 921.832857\n",
      "iteration 100 / 1500: loss 2.155281\n",
      "iteration 200 / 1500: loss 2.071084\n",
      "iteration 300 / 1500: loss 2.142339\n",
      "iteration 400 / 1500: loss 2.065891\n",
      "iteration 500 / 1500: loss 2.090376\n",
      "iteration 600 / 1500: loss 2.086898\n",
      "iteration 700 / 1500: loss 2.142319\n",
      "iteration 800 / 1500: loss 2.062466\n",
      "iteration 900 / 1500: loss 2.115431\n",
      "iteration 1000 / 1500: loss 2.109970\n",
      "iteration 1100 / 1500: loss 2.094505\n",
      "iteration 1200 / 1500: loss 2.101230\n",
      "iteration 1300 / 1500: loss 2.117445\n",
      "iteration 1400 / 1500: loss 2.150626\n",
      "iteration 0 / 1500: loss 1241.803583\n",
      "iteration 100 / 1500: loss 2.092742\n",
      "iteration 200 / 1500: loss 2.133379\n",
      "iteration 300 / 1500: loss 2.091309\n",
      "iteration 400 / 1500: loss 2.142889\n",
      "iteration 500 / 1500: loss 2.144012\n",
      "iteration 600 / 1500: loss 2.180615\n",
      "iteration 700 / 1500: loss 2.028416\n",
      "iteration 800 / 1500: loss 2.129617\n",
      "iteration 900 / 1500: loss 2.156801\n",
      "iteration 1000 / 1500: loss 2.085382\n",
      "iteration 1100 / 1500: loss 2.112617\n",
      "iteration 1200 / 1500: loss 2.138055\n",
      "iteration 1300 / 1500: loss 2.218637\n",
      "iteration 1400 / 1500: loss 2.170626\n",
      "iteration 0 / 1500: loss 1537.057499\n",
      "iteration 100 / 1500: loss 2.179612\n",
      "iteration 200 / 1500: loss 2.143594\n",
      "iteration 300 / 1500: loss 2.157618\n",
      "iteration 400 / 1500: loss 2.173497\n",
      "iteration 500 / 1500: loss 2.104197\n",
      "iteration 600 / 1500: loss 2.154099\n",
      "iteration 700 / 1500: loss 2.165130\n",
      "iteration 800 / 1500: loss 2.182646\n",
      "iteration 900 / 1500: loss 2.167613\n",
      "iteration 1000 / 1500: loss 2.114384\n",
      "iteration 1100 / 1500: loss 2.176506\n",
      "iteration 1200 / 1500: loss 2.204829\n",
      "iteration 1300 / 1500: loss 2.143151\n",
      "iteration 1400 / 1500: loss 2.193695\n",
      "iteration 0 / 1500: loss 313.467448\n",
      "iteration 100 / 1500: loss 20.271852\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 200 / 1500: loss 3.117886\n",
      "iteration 300 / 1500: loss 2.062376\n",
      "iteration 400 / 1500: loss 2.110315\n",
      "iteration 500 / 1500: loss 2.019971\n",
      "iteration 600 / 1500: loss 2.043077\n",
      "iteration 700 / 1500: loss 2.067924\n",
      "iteration 800 / 1500: loss 2.021273\n",
      "iteration 900 / 1500: loss 2.047672\n",
      "iteration 1000 / 1500: loss 2.032109\n",
      "iteration 1100 / 1500: loss 2.007173\n",
      "iteration 1200 / 1500: loss 2.041800\n",
      "iteration 1300 / 1500: loss 2.011077\n",
      "iteration 1400 / 1500: loss 1.977272\n",
      "iteration 0 / 1500: loss 621.823186\n",
      "iteration 100 / 1500: loss 4.141653\n",
      "iteration 200 / 1500: loss 2.067330\n",
      "iteration 300 / 1500: loss 2.144439\n",
      "iteration 400 / 1500: loss 2.090293\n",
      "iteration 500 / 1500: loss 2.010879\n",
      "iteration 600 / 1500: loss 2.068223\n",
      "iteration 700 / 1500: loss 2.078971\n",
      "iteration 800 / 1500: loss 2.078610\n",
      "iteration 900 / 1500: loss 2.081736\n",
      "iteration 1000 / 1500: loss 2.054624\n",
      "iteration 1100 / 1500: loss 2.058085\n",
      "iteration 1200 / 1500: loss 2.083574\n",
      "iteration 1300 / 1500: loss 2.101854\n",
      "iteration 1400 / 1500: loss 2.042034\n",
      "iteration 0 / 1500: loss 928.272463\n",
      "iteration 100 / 1500: loss 2.226161\n",
      "iteration 200 / 1500: loss 2.106195\n",
      "iteration 300 / 1500: loss 2.020245\n",
      "iteration 400 / 1500: loss 2.082419\n",
      "iteration 500 / 1500: loss 2.086540\n",
      "iteration 600 / 1500: loss 2.148795\n",
      "iteration 700 / 1500: loss 2.129100\n",
      "iteration 800 / 1500: loss 2.114482\n",
      "iteration 900 / 1500: loss 2.136321\n",
      "iteration 1000 / 1500: loss 2.124480\n",
      "iteration 1100 / 1500: loss 2.124404\n",
      "iteration 1200 / 1500: loss 2.149161\n",
      "iteration 1300 / 1500: loss 2.106630\n",
      "iteration 1400 / 1500: loss 2.049160\n",
      "iteration 0 / 1500: loss 1229.402868\n",
      "iteration 100 / 1500: loss 2.140697\n",
      "iteration 200 / 1500: loss 2.168665\n",
      "iteration 300 / 1500: loss 2.154136\n",
      "iteration 400 / 1500: loss 2.129367\n",
      "iteration 500 / 1500: loss 2.100684\n",
      "iteration 600 / 1500: loss 2.168451\n",
      "iteration 700 / 1500: loss 2.105707\n",
      "iteration 800 / 1500: loss 2.103277\n",
      "iteration 900 / 1500: loss 2.151624\n",
      "iteration 1000 / 1500: loss 2.159150\n",
      "iteration 1100 / 1500: loss 2.142593\n",
      "iteration 1200 / 1500: loss 2.121231\n",
      "iteration 1300 / 1500: loss 2.116418\n",
      "iteration 1400 / 1500: loss 2.113409\n",
      "iteration 0 / 1500: loss 1530.000004\n",
      "iteration 100 / 1500: loss 2.160309\n",
      "iteration 200 / 1500: loss 2.115258\n",
      "iteration 300 / 1500: loss 2.131182\n",
      "iteration 400 / 1500: loss 2.150850\n",
      "iteration 500 / 1500: loss 2.172105\n",
      "iteration 600 / 1500: loss 2.141481\n",
      "iteration 700 / 1500: loss 2.120872\n",
      "iteration 800 / 1500: loss 2.097038\n",
      "iteration 900 / 1500: loss 2.134639\n",
      "iteration 1000 / 1500: loss 2.168514\n",
      "iteration 1100 / 1500: loss 2.174412\n",
      "iteration 1200 / 1500: loss 2.143177\n",
      "iteration 1300 / 1500: loss 2.133704\n",
      "iteration 1400 / 1500: loss 2.122598\n",
      "iteration 0 / 1500: loss 308.739105\n",
      "iteration 100 / 1500: loss 28.878839\n",
      "iteration 200 / 1500: loss 4.394601\n",
      "iteration 300 / 1500: loss 2.198499\n",
      "iteration 400 / 1500: loss 2.126962\n",
      "iteration 500 / 1500: loss 1.944751\n",
      "iteration 600 / 1500: loss 2.059727\n",
      "iteration 700 / 1500: loss 2.031921\n",
      "iteration 800 / 1500: loss 1.985078\n",
      "iteration 900 / 1500: loss 1.991412\n",
      "iteration 1000 / 1500: loss 2.021876\n",
      "iteration 1100 / 1500: loss 1.962465\n",
      "iteration 1200 / 1500: loss 1.938086\n",
      "iteration 1300 / 1500: loss 1.973286\n",
      "iteration 1400 / 1500: loss 2.000707\n",
      "iteration 0 / 1500: loss 612.306465\n",
      "iteration 100 / 1500: loss 6.743524\n",
      "iteration 200 / 1500: loss 2.158743\n",
      "iteration 300 / 1500: loss 2.085612\n",
      "iteration 400 / 1500: loss 2.016668\n",
      "iteration 500 / 1500: loss 2.083560\n",
      "iteration 600 / 1500: loss 2.065239\n",
      "iteration 700 / 1500: loss 2.110745\n",
      "iteration 800 / 1500: loss 2.138660\n",
      "iteration 900 / 1500: loss 2.118718\n",
      "iteration 1000 / 1500: loss 2.042968\n",
      "iteration 1100 / 1500: loss 2.107876\n",
      "iteration 1200 / 1500: loss 2.113948\n",
      "iteration 1300 / 1500: loss 2.106364\n",
      "iteration 1400 / 1500: loss 2.068088\n",
      "iteration 0 / 1500: loss 931.788362\n",
      "iteration 100 / 1500: loss 2.722287\n",
      "iteration 200 / 1500: loss 2.127905\n",
      "iteration 300 / 1500: loss 2.088600\n",
      "iteration 400 / 1500: loss 2.117556\n",
      "iteration 500 / 1500: loss 2.073271\n",
      "iteration 600 / 1500: loss 2.132658\n",
      "iteration 700 / 1500: loss 2.091772\n",
      "iteration 800 / 1500: loss 2.122915\n",
      "iteration 900 / 1500: loss 2.131240\n",
      "iteration 1000 / 1500: loss 2.087007\n",
      "iteration 1100 / 1500: loss 2.047702\n",
      "iteration 1200 / 1500: loss 2.104399\n",
      "iteration 1300 / 1500: loss 2.147236\n",
      "iteration 1400 / 1500: loss 2.104686\n",
      "iteration 0 / 1500: loss 1230.281099\n",
      "iteration 100 / 1500: loss 2.221351\n",
      "iteration 200 / 1500: loss 2.106638\n",
      "iteration 300 / 1500: loss 2.133653\n",
      "iteration 400 / 1500: loss 2.141555\n",
      "iteration 500 / 1500: loss 2.160588\n",
      "iteration 600 / 1500: loss 2.158299\n",
      "iteration 700 / 1500: loss 2.109946\n",
      "iteration 800 / 1500: loss 2.136770\n",
      "iteration 900 / 1500: loss 2.100407\n",
      "iteration 1000 / 1500: loss 2.142876\n",
      "iteration 1100 / 1500: loss 2.102541\n",
      "iteration 1200 / 1500: loss 2.159913\n",
      "iteration 1300 / 1500: loss 2.103130\n",
      "iteration 1400 / 1500: loss 2.130851\n",
      "iteration 0 / 1500: loss 1525.692290\n",
      "iteration 100 / 1500: loss 2.130937\n",
      "iteration 200 / 1500: loss 2.173125\n",
      "iteration 300 / 1500: loss 2.116883\n",
      "iteration 400 / 1500: loss 2.185520\n",
      "iteration 500 / 1500: loss 2.112905\n",
      "iteration 600 / 1500: loss 2.153555\n",
      "iteration 700 / 1500: loss 2.104963\n",
      "iteration 800 / 1500: loss 2.155094\n",
      "iteration 900 / 1500: loss 2.158274\n",
      "iteration 1000 / 1500: loss 2.149004\n",
      "iteration 1100 / 1500: loss 2.163350\n",
      "iteration 1200 / 1500: loss 2.154337\n",
      "iteration 1300 / 1500: loss 2.093946\n",
      "iteration 1400 / 1500: loss 2.148250\n",
      "iteration 0 / 1500: loss 315.214112\n",
      "iteration 100 / 1500: loss 43.114119\n",
      "iteration 200 / 1500: loss 7.568913\n",
      "iteration 300 / 1500: loss 2.756251\n",
      "iteration 400 / 1500: loss 2.099131\n",
      "iteration 500 / 1500: loss 1.998087\n",
      "iteration 600 / 1500: loss 2.017375\n",
      "iteration 700 / 1500: loss 2.071403\n",
      "iteration 800 / 1500: loss 2.035186\n",
      "iteration 900 / 1500: loss 2.111676\n",
      "iteration 1000 / 1500: loss 1.969390\n",
      "iteration 1100 / 1500: loss 2.069323\n",
      "iteration 1200 / 1500: loss 1.950961\n",
      "iteration 1300 / 1500: loss 1.953710\n",
      "iteration 1400 / 1500: loss 2.008665\n",
      "iteration 0 / 1500: loss 618.929711\n",
      "iteration 100 / 1500: loss 12.734983\n",
      "iteration 200 / 1500: loss 2.261473\n",
      "iteration 300 / 1500: loss 2.089802\n",
      "iteration 400 / 1500: loss 2.048305\n",
      "iteration 500 / 1500: loss 2.000356\n",
      "iteration 600 / 1500: loss 2.026187\n",
      "iteration 700 / 1500: loss 2.094631\n",
      "iteration 800 / 1500: loss 1.989827\n",
      "iteration 900 / 1500: loss 2.039999\n",
      "iteration 1000 / 1500: loss 2.060605\n",
      "iteration 1100 / 1500: loss 2.117538\n",
      "iteration 1200 / 1500: loss 2.060436\n",
      "iteration 1300 / 1500: loss 2.106318\n",
      "iteration 1400 / 1500: loss 2.048951\n",
      "iteration 0 / 1500: loss 937.464552\n",
      "iteration 100 / 1500: loss 4.219174\n",
      "iteration 200 / 1500: loss 2.146329\n",
      "iteration 300 / 1500: loss 2.092519\n",
      "iteration 400 / 1500: loss 2.130305\n",
      "iteration 500 / 1500: loss 2.072885\n",
      "iteration 600 / 1500: loss 2.050887\n",
      "iteration 700 / 1500: loss 2.115872\n",
      "iteration 800 / 1500: loss 2.110827\n",
      "iteration 900 / 1500: loss 2.109481\n",
      "iteration 1000 / 1500: loss 2.117585\n",
      "iteration 1100 / 1500: loss 2.111971\n",
      "iteration 1200 / 1500: loss 2.122865\n",
      "iteration 1300 / 1500: loss 2.113301\n",
      "iteration 1400 / 1500: loss 2.119130\n",
      "iteration 0 / 1500: loss 1241.781322\n",
      "iteration 100 / 1500: loss 2.488104\n",
      "iteration 200 / 1500: loss 2.157846\n",
      "iteration 300 / 1500: loss 2.136130\n",
      "iteration 400 / 1500: loss 2.176956\n",
      "iteration 500 / 1500: loss 2.184118\n",
      "iteration 600 / 1500: loss 2.147223\n",
      "iteration 700 / 1500: loss 2.191891\n",
      "iteration 800 / 1500: loss 2.129620\n",
      "iteration 900 / 1500: loss 2.120223\n",
      "iteration 1000 / 1500: loss 2.163190\n",
      "iteration 1100 / 1500: loss 2.088799\n",
      "iteration 1200 / 1500: loss 2.141379\n",
      "iteration 1300 / 1500: loss 2.111003\n",
      "iteration 1400 / 1500: loss 2.132360\n",
      "iteration 0 / 1500: loss 1535.373834\n",
      "iteration 100 / 1500: loss 2.228413\n",
      "iteration 200 / 1500: loss 2.143946\n",
      "iteration 300 / 1500: loss 2.145187\n",
      "iteration 400 / 1500: loss 2.157819\n",
      "iteration 500 / 1500: loss 2.140390\n",
      "iteration 600 / 1500: loss 2.110204\n",
      "iteration 700 / 1500: loss 2.086513\n",
      "iteration 800 / 1500: loss 2.162825\n",
      "iteration 900 / 1500: loss 2.240572\n",
      "iteration 1000 / 1500: loss 2.135177\n",
      "iteration 1100 / 1500: loss 2.141183\n",
      "iteration 1200 / 1500: loss 2.131423\n",
      "iteration 1300 / 1500: loss 2.137660\n",
      "iteration 1400 / 1500: loss 2.168282\n",
      "iteration 0 / 1500: loss 315.405929\n",
      "iteration 100 / 1500: loss 63.664901\n",
      "iteration 200 / 1500: loss 14.242228\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 300 / 1500: loss 4.428216\n",
      "iteration 400 / 1500: loss 2.458437\n",
      "iteration 500 / 1500: loss 2.080498\n",
      "iteration 600 / 1500: loss 2.036749\n",
      "iteration 700 / 1500: loss 2.089785\n",
      "iteration 800 / 1500: loss 2.039671\n",
      "iteration 900 / 1500: loss 1.998769\n",
      "iteration 1000 / 1500: loss 1.979647\n",
      "iteration 1100 / 1500: loss 2.004799\n",
      "iteration 1200 / 1500: loss 1.946422\n",
      "iteration 1300 / 1500: loss 2.044959\n",
      "iteration 1400 / 1500: loss 1.968976\n",
      "iteration 0 / 1500: loss 623.780301\n",
      "iteration 100 / 1500: loss 26.390907\n",
      "iteration 200 / 1500: loss 3.077822\n",
      "iteration 300 / 1500: loss 2.133723\n",
      "iteration 400 / 1500: loss 1.971931\n",
      "iteration 500 / 1500: loss 2.015127\n",
      "iteration 600 / 1500: loss 2.030815\n",
      "iteration 700 / 1500: loss 2.026811\n",
      "iteration 800 / 1500: loss 2.089800\n",
      "iteration 900 / 1500: loss 1.999234\n",
      "iteration 1000 / 1500: loss 2.077436\n",
      "iteration 1100 / 1500: loss 2.037243\n",
      "iteration 1200 / 1500: loss 2.062674\n",
      "iteration 1300 / 1500: loss 2.067823\n",
      "iteration 1400 / 1500: loss 2.057068\n",
      "iteration 0 / 1500: loss 919.277173\n",
      "iteration 100 / 1500: loss 9.127224\n",
      "iteration 200 / 1500: loss 2.150375\n",
      "iteration 300 / 1500: loss 2.098433\n",
      "iteration 400 / 1500: loss 2.123564\n",
      "iteration 500 / 1500: loss 2.108337\n",
      "iteration 600 / 1500: loss 2.107877\n",
      "iteration 700 / 1500: loss 2.102086\n",
      "iteration 800 / 1500: loss 2.102915\n",
      "iteration 900 / 1500: loss 2.132044\n",
      "iteration 1000 / 1500: loss 2.086338\n",
      "iteration 1100 / 1500: loss 2.111107\n",
      "iteration 1200 / 1500: loss 2.047897\n",
      "iteration 1300 / 1500: loss 2.063715\n",
      "iteration 1400 / 1500: loss 2.086428\n",
      "iteration 0 / 1500: loss 1239.116103\n",
      "iteration 100 / 1500: loss 3.946885\n",
      "iteration 200 / 1500: loss 2.129531\n",
      "iteration 300 / 1500: loss 2.109074\n",
      "iteration 400 / 1500: loss 2.160469\n",
      "iteration 500 / 1500: loss 2.090067\n",
      "iteration 600 / 1500: loss 2.158647\n",
      "iteration 700 / 1500: loss 2.147270\n",
      "iteration 800 / 1500: loss 2.098906\n",
      "iteration 900 / 1500: loss 2.152247\n",
      "iteration 1000 / 1500: loss 2.144275\n",
      "iteration 1100 / 1500: loss 2.124792\n",
      "iteration 1200 / 1500: loss 2.126503\n",
      "iteration 1300 / 1500: loss 2.123871\n",
      "iteration 1400 / 1500: loss 2.151222\n",
      "iteration 0 / 1500: loss 1535.245184\n",
      "iteration 100 / 1500: loss 2.543966\n",
      "iteration 200 / 1500: loss 2.198301\n",
      "iteration 300 / 1500: loss 2.126831\n",
      "iteration 400 / 1500: loss 2.127604\n",
      "iteration 500 / 1500: loss 2.104660\n",
      "iteration 600 / 1500: loss 2.112301\n",
      "iteration 700 / 1500: loss 2.150740\n",
      "iteration 800 / 1500: loss 2.104396\n",
      "iteration 900 / 1500: loss 2.107686\n",
      "iteration 1000 / 1500: loss 2.124869\n",
      "iteration 1100 / 1500: loss 2.148062\n",
      "iteration 1200 / 1500: loss 2.126368\n",
      "iteration 1300 / 1500: loss 2.173245\n",
      "iteration 1400 / 1500: loss 2.206738\n",
      "iteration 0 / 1500: loss 317.349590\n",
      "iteration 100 / 1500: loss 95.400637\n",
      "iteration 200 / 1500: loss 29.854246\n",
      "iteration 300 / 1500: loss 10.265123\n",
      "iteration 400 / 1500: loss 4.479458\n",
      "iteration 500 / 1500: loss 2.781088\n",
      "iteration 600 / 1500: loss 2.256175\n",
      "iteration 700 / 1500: loss 2.063357\n",
      "iteration 800 / 1500: loss 1.993677\n",
      "iteration 900 / 1500: loss 2.041366\n",
      "iteration 1000 / 1500: loss 1.921685\n",
      "iteration 1100 / 1500: loss 1.916532\n",
      "iteration 1200 / 1500: loss 2.046795\n",
      "iteration 1300 / 1500: loss 2.047726\n",
      "iteration 1400 / 1500: loss 1.973734\n",
      "iteration 0 / 1500: loss 619.861873\n",
      "iteration 100 / 1500: loss 56.711957\n",
      "iteration 200 / 1500: loss 6.902066\n",
      "iteration 300 / 1500: loss 2.511655\n",
      "iteration 400 / 1500: loss 2.132648\n",
      "iteration 500 / 1500: loss 2.109946\n",
      "iteration 600 / 1500: loss 2.070738\n",
      "iteration 700 / 1500: loss 2.080152\n",
      "iteration 800 / 1500: loss 2.047808\n",
      "iteration 900 / 1500: loss 2.112632\n",
      "iteration 1000 / 1500: loss 2.073206\n",
      "iteration 1100 / 1500: loss 2.034964\n",
      "iteration 1200 / 1500: loss 2.081499\n",
      "iteration 1300 / 1500: loss 2.080088\n",
      "iteration 1400 / 1500: loss 2.066674\n",
      "iteration 0 / 1500: loss 938.809856\n",
      "iteration 100 / 1500: loss 26.669369\n",
      "iteration 200 / 1500: loss 2.788443\n",
      "iteration 300 / 1500: loss 2.131837\n",
      "iteration 400 / 1500: loss 2.067102\n",
      "iteration 500 / 1500: loss 2.180513\n",
      "iteration 600 / 1500: loss 2.106396\n",
      "iteration 700 / 1500: loss 2.100442\n",
      "iteration 800 / 1500: loss 2.093118\n",
      "iteration 900 / 1500: loss 2.064241\n",
      "iteration 1000 / 1500: loss 2.104388\n",
      "iteration 1100 / 1500: loss 2.107563\n",
      "iteration 1200 / 1500: loss 2.127992\n",
      "iteration 1300 / 1500: loss 2.055487\n",
      "iteration 1400 / 1500: loss 2.116948\n",
      "iteration 0 / 1500: loss 1221.921816\n",
      "iteration 100 / 1500: loss 11.562733\n",
      "iteration 200 / 1500: loss 2.163778\n",
      "iteration 300 / 1500: loss 2.137441\n",
      "iteration 400 / 1500: loss 2.064095\n",
      "iteration 500 / 1500: loss 2.100878\n",
      "iteration 600 / 1500: loss 2.133992\n",
      "iteration 700 / 1500: loss 2.124865\n",
      "iteration 800 / 1500: loss 2.156647\n",
      "iteration 900 / 1500: loss 2.125343\n",
      "iteration 1000 / 1500: loss 2.133128\n",
      "iteration 1100 / 1500: loss 2.133658\n",
      "iteration 1200 / 1500: loss 2.117918\n",
      "iteration 1300 / 1500: loss 2.171646\n",
      "iteration 1400 / 1500: loss 2.092523\n",
      "iteration 0 / 1500: loss 1549.255595\n",
      "iteration 100 / 1500: loss 5.589498\n",
      "iteration 200 / 1500: loss 2.121688\n",
      "iteration 300 / 1500: loss 2.208608\n",
      "iteration 400 / 1500: loss 2.152421\n",
      "iteration 500 / 1500: loss 2.157999\n",
      "iteration 600 / 1500: loss 2.158819\n",
      "iteration 700 / 1500: loss 2.113560\n",
      "iteration 800 / 1500: loss 2.144701\n",
      "iteration 900 / 1500: loss 2.124476\n",
      "iteration 1000 / 1500: loss 2.136733\n",
      "iteration 1100 / 1500: loss 2.166926\n",
      "iteration 1200 / 1500: loss 2.121125\n",
      "iteration 1300 / 1500: loss 2.135412\n",
      "iteration 1400 / 1500: loss 2.121208\n",
      "iteration 0 / 1500: loss 321.103554\n",
      "iteration 100 / 1500: loss 143.724052\n",
      "iteration 200 / 1500: loss 65.210835\n",
      "iteration 300 / 1500: loss 30.214412\n",
      "iteration 400 / 1500: loss 14.632208\n",
      "iteration 500 / 1500: loss 7.615854\n",
      "iteration 600 / 1500: loss 4.574981\n",
      "iteration 700 / 1500: loss 3.154619\n",
      "iteration 800 / 1500: loss 2.498683\n",
      "iteration 900 / 1500: loss 2.247543\n",
      "iteration 1000 / 1500: loss 2.123989\n",
      "iteration 1100 / 1500: loss 2.070912\n",
      "iteration 1200 / 1500: loss 1.962014\n",
      "iteration 1300 / 1500: loss 1.954058\n",
      "iteration 1400 / 1500: loss 1.993075\n",
      "iteration 0 / 1500: loss 626.416172\n",
      "iteration 100 / 1500: loss 126.318397\n",
      "iteration 200 / 1500: loss 26.875341\n",
      "iteration 300 / 1500: loss 7.041834\n",
      "iteration 400 / 1500: loss 3.100958\n",
      "iteration 500 / 1500: loss 2.217069\n",
      "iteration 600 / 1500: loss 2.030404\n",
      "iteration 700 / 1500: loss 2.062405\n",
      "iteration 800 / 1500: loss 1.995445\n",
      "iteration 900 / 1500: loss 2.024753\n",
      "iteration 1000 / 1500: loss 2.060661\n",
      "iteration 1100 / 1500: loss 2.038909\n",
      "iteration 1200 / 1500: loss 2.025149\n",
      "iteration 1300 / 1500: loss 2.070462\n",
      "iteration 1400 / 1500: loss 2.062852\n",
      "iteration 0 / 1500: loss 923.971931\n",
      "iteration 100 / 1500: loss 83.801373\n",
      "iteration 200 / 1500: loss 9.332652\n",
      "iteration 300 / 1500: loss 2.769788\n",
      "iteration 400 / 1500: loss 2.175344\n",
      "iteration 500 / 1500: loss 2.118847\n",
      "iteration 600 / 1500: loss 2.031608\n",
      "iteration 700 / 1500: loss 2.105931\n",
      "iteration 800 / 1500: loss 2.134980\n",
      "iteration 900 / 1500: loss 2.093814\n",
      "iteration 1000 / 1500: loss 2.110974\n",
      "iteration 1100 / 1500: loss 2.088287\n",
      "iteration 1200 / 1500: loss 2.031358\n",
      "iteration 1300 / 1500: loss 2.065904\n",
      "iteration 1400 / 1500: loss 2.069252\n",
      "iteration 0 / 1500: loss 1224.917095\n",
      "iteration 100 / 1500: loss 50.440609\n",
      "iteration 200 / 1500: loss 4.032685\n",
      "iteration 300 / 1500: loss 2.206512\n",
      "iteration 400 / 1500: loss 2.158180\n",
      "iteration 500 / 1500: loss 2.125455\n",
      "iteration 600 / 1500: loss 2.144744\n",
      "iteration 700 / 1500: loss 2.070320\n",
      "iteration 800 / 1500: loss 2.130903\n",
      "iteration 900 / 1500: loss 2.122202\n",
      "iteration 1000 / 1500: loss 2.116842\n",
      "iteration 1100 / 1500: loss 2.123470\n",
      "iteration 1200 / 1500: loss 2.142747\n",
      "iteration 1300 / 1500: loss 2.172233\n",
      "iteration 1400 / 1500: loss 2.173788\n",
      "iteration 0 / 1500: loss 1533.120518\n",
      "iteration 100 / 1500: loss 28.834006\n",
      "iteration 200 / 1500: loss 2.644370\n",
      "iteration 300 / 1500: loss 2.177091\n",
      "iteration 400 / 1500: loss 2.149807\n",
      "iteration 500 / 1500: loss 2.119554\n",
      "iteration 600 / 1500: loss 2.155712\n",
      "iteration 700 / 1500: loss 2.110855\n",
      "iteration 800 / 1500: loss 2.130513\n",
      "iteration 900 / 1500: loss 2.149628\n",
      "iteration 1000 / 1500: loss 2.176410\n",
      "iteration 1100 / 1500: loss 2.122913\n",
      "iteration 1200 / 1500: loss 2.165231\n",
      "iteration 1300 / 1500: loss 2.143487\n",
      "iteration 1400 / 1500: loss 2.175131\n",
      "iteration 0 / 1500: loss 313.026377\n",
      "iteration 100 / 1500: loss 208.495622\n",
      "iteration 200 / 1500: loss 140.099047\n",
      "iteration 300 / 1500: loss 94.209631\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 400 / 1500: loss 63.662334\n",
      "iteration 500 / 1500: loss 43.133064\n",
      "iteration 600 / 1500: loss 29.506044\n",
      "iteration 700 / 1500: loss 20.397024\n",
      "iteration 800 / 1500: loss 14.286968\n",
      "iteration 900 / 1500: loss 10.196848\n",
      "iteration 1000 / 1500: loss 7.478467\n",
      "iteration 1100 / 1500: loss 5.689147\n",
      "iteration 1200 / 1500: loss 4.492230\n",
      "iteration 1300 / 1500: loss 3.629129\n",
      "iteration 1400 / 1500: loss 3.136530\n",
      "iteration 0 / 1500: loss 619.210454\n",
      "iteration 100 / 1500: loss 277.393547\n",
      "iteration 200 / 1500: loss 125.279111\n",
      "iteration 300 / 1500: loss 57.306408\n",
      "iteration 400 / 1500: loss 26.770460\n",
      "iteration 500 / 1500: loss 13.083089\n",
      "iteration 600 / 1500: loss 7.077758\n",
      "iteration 700 / 1500: loss 4.297009\n",
      "iteration 800 / 1500: loss 3.040101\n",
      "iteration 900 / 1500: loss 2.499053\n",
      "iteration 1000 / 1500: loss 2.281009\n",
      "iteration 1100 / 1500: loss 2.110512\n",
      "iteration 1200 / 1500: loss 2.114955\n",
      "iteration 1300 / 1500: loss 2.089493\n",
      "iteration 1400 / 1500: loss 2.038304\n",
      "iteration 0 / 1500: loss 924.421917\n",
      "iteration 100 / 1500: loss 277.935433\n",
      "iteration 200 / 1500: loss 84.600958\n",
      "iteration 300 / 1500: loss 26.782449\n",
      "iteration 400 / 1500: loss 9.548773\n",
      "iteration 500 / 1500: loss 4.321158\n",
      "iteration 600 / 1500: loss 2.740119\n",
      "iteration 700 / 1500: loss 2.311052\n",
      "iteration 800 / 1500: loss 2.201806\n",
      "iteration 900 / 1500: loss 2.096062\n",
      "iteration 1000 / 1500: loss 2.175585\n",
      "iteration 1100 / 1500: loss 2.087959\n",
      "iteration 1200 / 1500: loss 2.111423\n",
      "iteration 1300 / 1500: loss 2.026602\n",
      "iteration 1400 / 1500: loss 2.103636\n",
      "iteration 0 / 1500: loss 1240.814406\n",
      "iteration 100 / 1500: loss 249.645982\n",
      "iteration 200 / 1500: loss 51.637713\n",
      "iteration 300 / 1500: loss 12.048338\n",
      "iteration 400 / 1500: loss 4.081998\n",
      "iteration 500 / 1500: loss 2.509422\n",
      "iteration 600 / 1500: loss 2.185970\n",
      "iteration 700 / 1500: loss 2.184332\n",
      "iteration 800 / 1500: loss 2.143882\n",
      "iteration 900 / 1500: loss 2.158046\n",
      "iteration 1000 / 1500: loss 2.145957\n",
      "iteration 1100 / 1500: loss 2.118696\n",
      "iteration 1200 / 1500: loss 2.103339\n",
      "iteration 1300 / 1500: loss 2.116867\n",
      "iteration 1400 / 1500: loss 2.158336\n",
      "iteration 0 / 1500: loss 1546.020957\n",
      "iteration 100 / 1500: loss 208.340551\n",
      "iteration 200 / 1500: loss 29.662817\n",
      "iteration 300 / 1500: loss 5.811064\n",
      "iteration 400 / 1500: loss 2.622171\n",
      "iteration 500 / 1500: loss 2.224158\n",
      "iteration 600 / 1500: loss 2.181604\n",
      "iteration 700 / 1500: loss 2.114068\n",
      "iteration 800 / 1500: loss 2.172010\n",
      "iteration 900 / 1500: loss 2.132271\n",
      "iteration 1000 / 1500: loss 2.143812\n",
      "iteration 1100 / 1500: loss 2.127538\n",
      "iteration 1200 / 1500: loss 2.185657\n",
      "iteration 1300 / 1500: loss 2.097944\n",
      "iteration 1400 / 1500: loss 2.136799\n",
      "lr 1.000000e-07 reg 1.000000e+04 train accuracy: 0.348959 val accuracy: 0.370000\n",
      "lr 1.000000e-07 reg 2.000000e+04 train accuracy: 0.337694 val accuracy: 0.349000\n",
      "lr 1.000000e-07 reg 3.000000e+04 train accuracy: 0.318531 val accuracy: 0.338000\n",
      "lr 1.000000e-07 reg 4.000000e+04 train accuracy: 0.315184 val accuracy: 0.339000\n",
      "lr 1.000000e-07 reg 5.000000e+04 train accuracy: 0.304980 val accuracy: 0.322000\n",
      "lr 2.000000e-07 reg 1.000000e+04 train accuracy: 0.361143 val accuracy: 0.373000\n",
      "lr 2.000000e-07 reg 2.000000e+04 train accuracy: 0.338837 val accuracy: 0.353000\n",
      "lr 2.000000e-07 reg 3.000000e+04 train accuracy: 0.323082 val accuracy: 0.345000\n",
      "lr 2.000000e-07 reg 4.000000e+04 train accuracy: 0.318082 val accuracy: 0.329000\n",
      "lr 2.000000e-07 reg 5.000000e+04 train accuracy: 0.301551 val accuracy: 0.325000\n",
      "lr 3.000000e-07 reg 1.000000e+04 train accuracy: 0.356041 val accuracy: 0.372000\n",
      "lr 3.000000e-07 reg 2.000000e+04 train accuracy: 0.344429 val accuracy: 0.352000\n",
      "lr 3.000000e-07 reg 3.000000e+04 train accuracy: 0.319347 val accuracy: 0.348000\n",
      "lr 3.000000e-07 reg 4.000000e+04 train accuracy: 0.309857 val accuracy: 0.327000\n",
      "lr 3.000000e-07 reg 5.000000e+04 train accuracy: 0.307020 val accuracy: 0.314000\n",
      "lr 4.000000e-07 reg 1.000000e+04 train accuracy: 0.354592 val accuracy: 0.376000\n",
      "lr 4.000000e-07 reg 2.000000e+04 train accuracy: 0.338327 val accuracy: 0.348000\n",
      "lr 4.000000e-07 reg 3.000000e+04 train accuracy: 0.314204 val accuracy: 0.328000\n",
      "lr 4.000000e-07 reg 4.000000e+04 train accuracy: 0.310776 val accuracy: 0.332000\n",
      "lr 4.000000e-07 reg 5.000000e+04 train accuracy: 0.292388 val accuracy: 0.306000\n",
      "lr 5.000000e-07 reg 1.000000e+04 train accuracy: 0.352837 val accuracy: 0.351000\n",
      "lr 5.000000e-07 reg 2.000000e+04 train accuracy: 0.336082 val accuracy: 0.344000\n",
      "lr 5.000000e-07 reg 3.000000e+04 train accuracy: 0.316755 val accuracy: 0.324000\n",
      "lr 5.000000e-07 reg 4.000000e+04 train accuracy: 0.314653 val accuracy: 0.313000\n",
      "lr 5.000000e-07 reg 5.000000e+04 train accuracy: 0.286776 val accuracy: 0.306000\n",
      "lr 6.000000e-07 reg 1.000000e+04 train accuracy: 0.355143 val accuracy: 0.371000\n",
      "lr 6.000000e-07 reg 2.000000e+04 train accuracy: 0.331490 val accuracy: 0.345000\n",
      "lr 6.000000e-07 reg 3.000000e+04 train accuracy: 0.318755 val accuracy: 0.339000\n",
      "lr 6.000000e-07 reg 4.000000e+04 train accuracy: 0.302020 val accuracy: 0.326000\n",
      "lr 6.000000e-07 reg 5.000000e+04 train accuracy: 0.288286 val accuracy: 0.301000\n",
      "lr 7.000000e-07 reg 1.000000e+04 train accuracy: 0.351490 val accuracy: 0.357000\n",
      "lr 7.000000e-07 reg 2.000000e+04 train accuracy: 0.330694 val accuracy: 0.335000\n",
      "lr 7.000000e-07 reg 3.000000e+04 train accuracy: 0.315327 val accuracy: 0.329000\n",
      "lr 7.000000e-07 reg 4.000000e+04 train accuracy: 0.301082 val accuracy: 0.306000\n",
      "lr 7.000000e-07 reg 5.000000e+04 train accuracy: 0.285939 val accuracy: 0.306000\n",
      "lr 8.000000e-07 reg 1.000000e+04 train accuracy: 0.346327 val accuracy: 0.359000\n",
      "lr 8.000000e-07 reg 2.000000e+04 train accuracy: 0.338408 val accuracy: 0.340000\n",
      "lr 8.000000e-07 reg 3.000000e+04 train accuracy: 0.310510 val accuracy: 0.325000\n",
      "lr 8.000000e-07 reg 4.000000e+04 train accuracy: 0.300898 val accuracy: 0.326000\n",
      "lr 8.000000e-07 reg 5.000000e+04 train accuracy: 0.299347 val accuracy: 0.322000\n",
      "lr 9.000000e-07 reg 1.000000e+04 train accuracy: 0.344082 val accuracy: 0.367000\n",
      "lr 9.000000e-07 reg 2.000000e+04 train accuracy: 0.337041 val accuracy: 0.349000\n",
      "lr 9.000000e-07 reg 3.000000e+04 train accuracy: 0.310143 val accuracy: 0.322000\n",
      "lr 9.000000e-07 reg 4.000000e+04 train accuracy: 0.310388 val accuracy: 0.317000\n",
      "lr 9.000000e-07 reg 5.000000e+04 train accuracy: 0.299143 val accuracy: 0.294000\n",
      "lr 1.000000e-06 reg 1.000000e+04 train accuracy: 0.335714 val accuracy: 0.353000\n",
      "lr 1.000000e-06 reg 2.000000e+04 train accuracy: 0.327612 val accuracy: 0.338000\n",
      "lr 1.000000e-06 reg 3.000000e+04 train accuracy: 0.316102 val accuracy: 0.334000\n",
      "lr 1.000000e-06 reg 4.000000e+04 train accuracy: 0.293531 val accuracy: 0.297000\n",
      "lr 1.000000e-06 reg 5.000000e+04 train accuracy: 0.298469 val accuracy: 0.307000\n",
      "best validation accuracy achieved during cross-validation: 0.376000\n"
     ]
    }
   ],
   "source": [
    "# Use the validation set to tune hyperparameters (regularization strength and\n",
    "# learning rate). You should experiment with different ranges for the learning\n",
    "# rates and regularization strengths; if you are careful you should be able to\n",
    "# get a classification accuracy of over 0.35 on the validation set.\n",
    "from cs231n.classifiers import Softmax\n",
    "results = {}\n",
    "best_val = -1\n",
    "best_softmax = None\n",
    "learning_rates = [1e-6, 1e-7]\n",
    "regularization_strengths = [1e4, 5e4]\n",
    "\n",
    "################################################################################\n",
    "# TODO:                                                                        #\n",
    "# Use the validation set to set the learning rate and regularization strength. #\n",
    "# This should be identical to the validation that you did for the SVM; save    #\n",
    "# the best trained softmax classifer in best_softmax.                          #\n",
    "################################################################################\n",
    "import copy\n",
    "num_LR_tests = 10\n",
    "num_RS_tests = 5\n",
    "iters_per_test = 1500\n",
    "deltaLR = (learning_rates[1]-learning_rates[0])/(num_LR_tests-1)\n",
    "deltaRS = (regularization_strengths[1]-regularization_strengths[0])/(num_RS_tests-1)\n",
    "LR = learning_rates[0]\n",
    "for i in range(num_LR_tests):\n",
    "    RS = regularization_strengths[0]\n",
    "    for j in range(num_RS_tests):\n",
    "        testModel = Softmax()\n",
    "        testModel.train(X_train, y_train, learning_rate=LR, reg=RS, num_iters=iters_per_test, verbose=True)\n",
    "        y_train_pred = testModel.predict(X_train)\n",
    "        y_val_pred = testModel.predict(X_val)\n",
    "        val_acc = np.mean(y_val == y_val_pred)\n",
    "        if val_acc > best_val:\n",
    "            best_val = val_acc\n",
    "            best_softmax = copy.deepcopy(testModel)\n",
    "        results[(LR, RS)] = (np.mean(y_train == y_train_pred), val_acc)\n",
    "        RS += deltaRS\n",
    "    LR += deltaLR\n",
    "################################################################################\n",
    "#                              END OF YOUR CODE                                #\n",
    "################################################################################\n",
    "    \n",
    "# Print out results.\n",
    "for lr, reg in sorted(results):\n",
    "    train_accuracy, val_accuracy = results[(lr, reg)]\n",
    "    print('lr %e reg %e train accuracy: %f val accuracy: %f' % (\n",
    "                lr, reg, train_accuracy, val_accuracy))\n",
    "    \n",
    "print('best validation accuracy achieved during cross-validation: %f' % best_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "softmax on raw pixels final test set accuracy: 0.355000\n"
     ]
    }
   ],
   "source": [
    "# evaluate on test set\n",
    "# Evaluate the best softmax on test set\n",
    "y_test_pred = best_softmax.predict(X_test)\n",
    "test_accuracy = np.mean(y_test == y_test_pred)\n",
    "print('softmax on raw pixels final test set accuracy: %f' % (test_accuracy, ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Inline Question** - *True or False*\n",
    "\n",
    "It's possible to add a new datapoint to a training set that would leave the SVM loss unchanged, but this is not the case with the Softmax classifier loss.\n",
    "\n",
    "*Your answer*: \n",
    "\n",
    "*Your explanation*: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x576 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize the learned weights for each class\n",
    "w = best_softmax.W[:-1,:] # strip out the bias\n",
    "w = w.reshape(32, 32, 3, 10)\n",
    "\n",
    "w_min, w_max = np.min(w), np.max(w)\n",
    "\n",
    "classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "for i in range(10):\n",
    "    plt.subplot(2, 5, i + 1)\n",
    "    \n",
    "    # Rescale the weights to be between 0 and 255\n",
    "    wimg = 255.0 * (w[:, :, :, i].squeeze() - w_min) / (w_max - w_min)\n",
    "    plt.imshow(wimg.astype('uint8'))\n",
    "    plt.axis('off')\n",
    "    plt.title(classes[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
